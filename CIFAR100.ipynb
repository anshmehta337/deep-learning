{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0VYPCMCnxzeKOQPmfBke2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshmehta337/deep-learning/blob/main/CIFAR100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ztqwZfkyFME2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform=transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "FOxVjCxhIIox"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "uruEr1PmIrLk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6GjN3ZWJSNA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=dataset.CIFAR100(root='./data',train=True,download=True,transform=train_transform)\n",
        "test_dataset=dataset.CIFAR100(root='./data',train=False,download=True,transform=test_transform)\n"
      ],
      "metadata": {
        "id": "eA7RGChuI-YX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True,num_workers=4)\n",
        "\n",
        "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False,num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHFx-f5YJuwt",
        "outputId": "1b6a0f70-a97b-4147-bfa3-fd213730be71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets make resnet architecture"
      ],
      "metadata": {
        "id": "AHOD9ekaKECa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class build_res(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,drop=0.0,down_sample=False):\n",
        "    super().__init__()\n",
        "    stride=2 if down_sample else 1\n",
        "    self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
        "    self.bn1=nn.BatchNorm2d(out_channels)\n",
        "    self.relu=nn.ReLU(inplace=True)\n",
        "\n",
        "    self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "    self.bn2=nn.BatchNorm2d(out_channels)\n",
        "    self.drop=nn.Dropout(drop)\n",
        "\n",
        "    self.shortcut=nn.Sequential()\n",
        "    if in_channels!=out_channels or down_sample:\n",
        "      self.shortcut=nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "  def forward(self,x):\n",
        "      store=x\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x=self.relu(x)\n",
        "      x=self.conv2(x)\n",
        "      x=self.bn2(x)\n",
        "      x=self.drop(x)\n",
        "      x+=self.shortcut(store)\n",
        "      x=self.relu(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "YFXciw7IN-18"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class godeep(nn.Module):\n",
        "  def __init__(self,n_class=100):\n",
        "    super().__init__()\n",
        "    self.block1=nn.Sequential(\n",
        "        build_res(3,64,drop=0.2,down_sample=False),\n",
        "        build_res(64,64,drop=0.2),\n",
        "        build_res(64,64,drop=0.2),\n",
        "    )\n",
        "    self.block2=nn.Sequential(\n",
        "        build_res(64,128,drop=0.2,down_sample=True),\n",
        "        build_res(128,128,drop=0.2),\n",
        "        build_res(128,128,drop=0.2),\n",
        "        build_res(128,128,drop=0.2),\n",
        "    )\n",
        "    self.block3=nn.Sequential(\n",
        "         build_res(128,256,drop=0.3,down_sample=True),\n",
        "         build_res(256,256,drop=0.3),\n",
        "         build_res(256,256,drop=0.3),\n",
        "         build_res(256,256,drop=0.3),\n",
        "         build_res(256,256,drop=0.3),\n",
        "    )\n",
        "    self.block4=nn.Sequential(\n",
        "        build_res(256,512,drop=0.3,down_sample=True),\n",
        "        build_res(512,512,drop=0.3),\n",
        "        build_res(512,512,drop=0.3),\n",
        "    )\n",
        "    #pred now\n",
        "    self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc=nn.Sequential(\n",
        "         nn.Flatten(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 100)\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "      x=self.block1(x)\n",
        "      x=self.block2(x)\n",
        "      x=self.block3(x)\n",
        "      x=self.block4(x)\n",
        "      x=self.avgpool(x)\n",
        "      x=self.fc(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "ujjZh17GRk_H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now train our network\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "3UEgWWoOTp4p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = godeep().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n"
      ],
      "metadata": {
        "id": "wvsLVb9LTsSC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {running_loss/total:.4f} | Train Acc: {100*correct/total:.2f}%\")\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss_sum += criterion(outputs, targets).item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return loss_sum / total, 100 * correct / total\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "    train_one_epoch(epoch)\n",
        "    val_loss, val_acc = evaluate()\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y0OTxJ5T4A1",
        "outputId": "c3ccc703-ee47-4023-fbeb-7c0c28f64399"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train Loss: 4.3266 | Train Acc: 3.16%\n",
            "Validation Loss: 4.0796 | Validation Acc: 5.01%\n",
            "Epoch 1 | Train Loss: 4.0648 | Train Acc: 5.48%\n",
            "Validation Loss: 3.9200 | Validation Acc: 7.58%\n",
            "Epoch 2 | Train Loss: 3.8922 | Train Acc: 7.68%\n",
            "Validation Loss: 3.7608 | Validation Acc: 10.50%\n",
            "Epoch 3 | Train Loss: 3.7291 | Train Acc: 10.13%\n",
            "Validation Loss: 3.5654 | Validation Acc: 13.34%\n",
            "Epoch 4 | Train Loss: 3.5472 | Train Acc: 13.10%\n",
            "Validation Loss: 3.2815 | Validation Acc: 17.93%\n",
            "Epoch 5 | Train Loss: 3.3584 | Train Acc: 16.52%\n",
            "Validation Loss: 3.0904 | Validation Acc: 21.13%\n",
            "Epoch 6 | Train Loss: 3.1369 | Train Acc: 20.46%\n",
            "Validation Loss: 2.8321 | Validation Acc: 26.05%\n",
            "Epoch 7 | Train Loss: 2.9450 | Train Acc: 24.02%\n",
            "Validation Loss: 2.8769 | Validation Acc: 25.70%\n",
            "Epoch 8 | Train Loss: 2.7660 | Train Acc: 27.78%\n",
            "Validation Loss: 2.7195 | Validation Acc: 30.11%\n",
            "Epoch 9 | Train Loss: 2.6101 | Train Acc: 31.04%\n",
            "Validation Loss: 2.9668 | Validation Acc: 27.94%\n",
            "Epoch 10 | Train Loss: 2.4848 | Train Acc: 33.88%\n",
            "Validation Loss: 2.4256 | Validation Acc: 35.55%\n",
            "Epoch 11 | Train Loss: 2.3783 | Train Acc: 36.54%\n",
            "Validation Loss: 2.3419 | Validation Acc: 37.85%\n",
            "Epoch 12 | Train Loss: 2.3047 | Train Acc: 38.34%\n",
            "Validation Loss: 2.1327 | Validation Acc: 42.11%\n",
            "Epoch 13 | Train Loss: 2.2235 | Train Acc: 40.36%\n",
            "Validation Loss: 2.2719 | Validation Acc: 41.65%\n",
            "Epoch 14 | Train Loss: 2.1708 | Train Acc: 41.74%\n",
            "Validation Loss: 2.3176 | Validation Acc: 39.13%\n",
            "Epoch 15 | Train Loss: 2.0977 | Train Acc: 43.44%\n",
            "Validation Loss: 2.0313 | Validation Acc: 45.53%\n",
            "Epoch 16 | Train Loss: 2.0575 | Train Acc: 44.69%\n",
            "Validation Loss: 2.0752 | Validation Acc: 45.10%\n",
            "Epoch 17 | Train Loss: 2.0084 | Train Acc: 46.04%\n",
            "Validation Loss: 2.0441 | Validation Acc: 46.28%\n",
            "Epoch 18 | Train Loss: 1.9736 | Train Acc: 46.80%\n",
            "Validation Loss: 2.1664 | Validation Acc: 45.01%\n",
            "Epoch 19 | Train Loss: 1.9283 | Train Acc: 48.23%\n",
            "Validation Loss: 2.0212 | Validation Acc: 47.34%\n",
            "Epoch 20 | Train Loss: 1.8974 | Train Acc: 48.98%\n",
            "Validation Loss: 1.9959 | Validation Acc: 47.99%\n",
            "Epoch 21 | Train Loss: 1.8575 | Train Acc: 50.08%\n",
            "Validation Loss: 1.7982 | Validation Acc: 51.62%\n",
            "Epoch 22 | Train Loss: 1.8369 | Train Acc: 50.90%\n",
            "Validation Loss: 1.8299 | Validation Acc: 51.45%\n",
            "Epoch 23 | Train Loss: 1.8105 | Train Acc: 51.55%\n",
            "Validation Loss: 1.8166 | Validation Acc: 52.24%\n",
            "Epoch 24 | Train Loss: 1.7860 | Train Acc: 52.20%\n",
            "Validation Loss: 1.8588 | Validation Acc: 51.69%\n",
            "Epoch 25 | Train Loss: 1.7566 | Train Acc: 52.83%\n",
            "Validation Loss: 1.8085 | Validation Acc: 52.47%\n",
            "Epoch 26 | Train Loss: 1.7271 | Train Acc: 53.68%\n",
            "Validation Loss: 1.9000 | Validation Acc: 51.52%\n",
            "Epoch 27 | Train Loss: 1.7032 | Train Acc: 54.40%\n",
            "Validation Loss: 1.8034 | Validation Acc: 52.76%\n",
            "Epoch 28 | Train Loss: 1.6822 | Train Acc: 54.99%\n",
            "Validation Loss: 1.8294 | Validation Acc: 53.19%\n",
            "Epoch 29 | Train Loss: 1.6611 | Train Acc: 55.48%\n",
            "Validation Loss: 1.7652 | Validation Acc: 53.86%\n",
            "Epoch 30 | Train Loss: 1.6357 | Train Acc: 56.08%\n",
            "Validation Loss: 1.7475 | Validation Acc: 54.25%\n",
            "Epoch 31 | Train Loss: 1.6218 | Train Acc: 56.58%\n",
            "Validation Loss: 1.6619 | Validation Acc: 56.43%\n",
            "Epoch 32 | Train Loss: 1.5966 | Train Acc: 56.90%\n",
            "Validation Loss: 1.9303 | Validation Acc: 52.48%\n",
            "Epoch 33 | Train Loss: 1.5682 | Train Acc: 57.85%\n",
            "Validation Loss: 1.7411 | Validation Acc: 54.92%\n",
            "Epoch 34 | Train Loss: 1.5419 | Train Acc: 58.83%\n",
            "Validation Loss: 1.8815 | Validation Acc: 52.54%\n",
            "Epoch 35 | Train Loss: 1.5305 | Train Acc: 59.10%\n",
            "Validation Loss: 1.7952 | Validation Acc: 54.76%\n",
            "Epoch 36 | Train Loss: 1.5036 | Train Acc: 59.35%\n",
            "Validation Loss: 1.7591 | Validation Acc: 55.25%\n",
            "Epoch 37 | Train Loss: 1.4920 | Train Acc: 59.92%\n",
            "Validation Loss: 1.7705 | Validation Acc: 55.06%\n",
            "Epoch 38 | Train Loss: 1.4663 | Train Acc: 60.78%\n",
            "Validation Loss: 1.6389 | Validation Acc: 57.25%\n",
            "Epoch 39 | Train Loss: 1.4371 | Train Acc: 61.45%\n",
            "Validation Loss: 1.7646 | Validation Acc: 56.15%\n",
            "Epoch 40 | Train Loss: 1.4320 | Train Acc: 61.51%\n",
            "Validation Loss: 1.6078 | Validation Acc: 57.83%\n",
            "Epoch 41 | Train Loss: 1.4037 | Train Acc: 62.36%\n",
            "Validation Loss: 1.7524 | Validation Acc: 56.09%\n",
            "Epoch 42 | Train Loss: 1.3793 | Train Acc: 62.98%\n",
            "Validation Loss: 1.6162 | Validation Acc: 58.25%\n",
            "Epoch 43 | Train Loss: 1.3622 | Train Acc: 63.74%\n",
            "Validation Loss: 1.7365 | Validation Acc: 56.41%\n",
            "Epoch 44 | Train Loss: 1.3506 | Train Acc: 63.76%\n",
            "Validation Loss: 1.4876 | Validation Acc: 60.45%\n",
            "Epoch 45 | Train Loss: 1.3056 | Train Acc: 64.95%\n",
            "Validation Loss: 1.5995 | Validation Acc: 59.27%\n",
            "Epoch 46 | Train Loss: 1.2797 | Train Acc: 65.39%\n",
            "Validation Loss: 1.7379 | Validation Acc: 58.42%\n",
            "Epoch 47 | Train Loss: 1.2729 | Train Acc: 65.83%\n",
            "Validation Loss: 1.5675 | Validation Acc: 59.77%\n",
            "Epoch 48 | Train Loss: 1.2327 | Train Acc: 66.75%\n",
            "Validation Loss: 1.6045 | Validation Acc: 60.38%\n",
            "Epoch 49 | Train Loss: 1.2299 | Train Acc: 67.00%\n",
            "Validation Loss: 1.6190 | Validation Acc: 59.46%\n",
            "Epoch 50 | Train Loss: 1.1859 | Train Acc: 67.98%\n",
            "Validation Loss: 1.4674 | Validation Acc: 61.21%\n",
            "Epoch 51 | Train Loss: 1.1661 | Train Acc: 68.31%\n",
            "Validation Loss: 1.5324 | Validation Acc: 61.40%\n",
            "Epoch 52 | Train Loss: 1.1433 | Train Acc: 69.18%\n",
            "Validation Loss: 1.5883 | Validation Acc: 60.12%\n",
            "Epoch 53 | Train Loss: 1.1200 | Train Acc: 69.62%\n",
            "Validation Loss: 1.6081 | Validation Acc: 60.05%\n",
            "Epoch 54 | Train Loss: 1.0786 | Train Acc: 70.76%\n",
            "Validation Loss: 1.5989 | Validation Acc: 60.76%\n",
            "Epoch 55 | Train Loss: 1.0721 | Train Acc: 70.82%\n",
            "Validation Loss: 1.5141 | Validation Acc: 61.60%\n",
            "Epoch 56 | Train Loss: 1.0323 | Train Acc: 71.70%\n",
            "Validation Loss: 1.5611 | Validation Acc: 61.05%\n",
            "Epoch 57 | Train Loss: 1.0157 | Train Acc: 72.45%\n",
            "Validation Loss: 1.5337 | Validation Acc: 62.22%\n",
            "Epoch 58 | Train Loss: 0.9840 | Train Acc: 73.27%\n",
            "Validation Loss: 1.4222 | Validation Acc: 63.91%\n",
            "Epoch 59 | Train Loss: 0.9441 | Train Acc: 74.24%\n",
            "Validation Loss: 1.4545 | Validation Acc: 63.57%\n",
            "Epoch 60 | Train Loss: 0.9290 | Train Acc: 74.49%\n",
            "Validation Loss: 1.4371 | Validation Acc: 65.17%\n",
            "Epoch 61 | Train Loss: 0.8950 | Train Acc: 75.52%\n",
            "Validation Loss: 1.4588 | Validation Acc: 63.61%\n",
            "Epoch 62 | Train Loss: 0.8554 | Train Acc: 76.38%\n",
            "Validation Loss: 1.4360 | Validation Acc: 64.52%\n",
            "Epoch 63 | Train Loss: 0.8298 | Train Acc: 77.24%\n",
            "Validation Loss: 1.4437 | Validation Acc: 64.66%\n",
            "Epoch 64 | Train Loss: 0.8012 | Train Acc: 77.85%\n",
            "Validation Loss: 1.3597 | Validation Acc: 66.26%\n",
            "Epoch 65 | Train Loss: 0.7658 | Train Acc: 78.91%\n",
            "Validation Loss: 1.3344 | Validation Acc: 66.81%\n",
            "Epoch 66 | Train Loss: 0.7303 | Train Acc: 79.79%\n",
            "Validation Loss: 1.3909 | Validation Acc: 65.90%\n",
            "Epoch 67 | Train Loss: 0.7077 | Train Acc: 80.37%\n",
            "Validation Loss: 1.5117 | Validation Acc: 64.71%\n",
            "Epoch 68 | Train Loss: 0.6670 | Train Acc: 81.57%\n",
            "Validation Loss: 1.3973 | Validation Acc: 66.56%\n",
            "Epoch 69 | Train Loss: 0.6305 | Train Acc: 82.37%\n",
            "Validation Loss: 1.3824 | Validation Acc: 67.18%\n",
            "Epoch 70 | Train Loss: 0.5910 | Train Acc: 83.35%\n",
            "Validation Loss: 1.4377 | Validation Acc: 66.96%\n",
            "Epoch 71 | Train Loss: 0.5620 | Train Acc: 84.34%\n",
            "Validation Loss: 1.3703 | Validation Acc: 67.88%\n",
            "Epoch 72 | Train Loss: 0.5257 | Train Acc: 85.19%\n",
            "Validation Loss: 1.3885 | Validation Acc: 67.44%\n",
            "Epoch 73 | Train Loss: 0.4974 | Train Acc: 86.08%\n",
            "Validation Loss: 1.3418 | Validation Acc: 68.66%\n",
            "Epoch 74 | Train Loss: 0.4561 | Train Acc: 87.02%\n",
            "Validation Loss: 1.3742 | Validation Acc: 68.90%\n",
            "Epoch 75 | Train Loss: 0.4235 | Train Acc: 88.00%\n",
            "Validation Loss: 1.3726 | Validation Acc: 68.44%\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    }
  ]
}